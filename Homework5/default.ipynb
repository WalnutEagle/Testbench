{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgNCuxS0OZu4"
      },
      "source": [
        "## EC523 HW#5: Music Generation with LSTMs\n",
        "\n",
        "In this homework, you will be applying autoregressive models to the problem of generating symbolic music."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLkUnMl0vZwm"
      },
      "outputs": [],
      "source": [
        "!sudo apt install -y fluidsynth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il6HPxshvcl0"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pyfluidsynth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX7nh3UBvec0"
      },
      "outputs": [],
      "source": [
        "!pip install pretty_midi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKBQHoWRoGHi"
      },
      "outputs": [],
      "source": [
        "### IMPORTS ###\n",
        "import os\n",
        "import librosa\n",
        "import pretty_midi\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import statistics\n",
        "from IPython.display import Audio\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDIPn22C7qbI"
      },
      "source": [
        "### Part 1: Loading & Understanding the Data (12 points, 2 per part)\n",
        "The dataset we will be using is a subset of the [Lakh MIDI Dataset](https://colinraffel.com/projects/lmd/). MIDI (Musical Instrument Digital Interface) is a widely used communication protocol and file format that allows communication between electronic musical instruments and computers. It essentially stores the same information that is communicated in sheet music (what notes play when and how loudly). As opposed to a wav file, which stores the actual waveform of a musical performance, MIDI files carry much less information and thus are significantly smaller.\n",
        "\n",
        "The dataset you will be using for this homework has been filtered and preprocessed in the following ways:\n",
        "- Standardized to the same tempo (120bpm) and key (C/Am)\n",
        "- Each MIDI file contains exactly one (non-drum) track\n",
        "- Dataset only contains monophonic tracks, i.e. there is a maximum of one note playing at any given time.\n",
        "- Removed excessively sparse tracks (tracks with a note playing <30% of the time)\n",
        "\n",
        "We can use the [pretty_midi](https://craffel.github.io/pretty-midi/) library to manipulate MIDI files in Python. In this section of the homework, you will familiarize yourself with the training data and ways to view, listen to, and manipulate it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRbM0170NJUK"
      },
      "source": [
        "**Problem 1:** The dataset is uploaded to this scc location: `/projectnb/ec523kb/materials/Copy of ec523_MIDI_DATA.zip`. Unzip it first. Load a pretty_midi object for the file 'ABBA_Mamma_Mia_t7.mid' and answer the following questions about it:\n",
        "\n",
        "1.1 How long is the MIDI in seconds?\n",
        "\n",
        "1.2 What is the name of the instrument that the track is set to?\n",
        "\n",
        "1.3 How many total notes are played?\n",
        "\n",
        "1.4 What are the names of the pitches of the first 10 notes in the MIDI?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdrFzMpytH8A"
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Consult the pretty_midi documentation page for help with answering any of these questions :)\n",
        "data_directory = '/path/to/midi/data/'\n",
        "pm = pretty_midi.PrettyMIDI(data_directory + 'ABBA_Mamma_Mia_t7.mid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFy-i0jtmwuN"
      },
      "source": [
        "Now that we understand the PrettyMIDI class a little bit better, it would also be helpful for us to be able to visualize and listen to the MIDI data.\n",
        "\n",
        "For visualization, we first need to convert our MIDI object into a **piano roll**. A piano roll is a 2-D representation of MIDI where the X axis represents time and the Y axis represents pitch. When we represent MIDI this way, we can easily graph it. Luckily, the pretty_midi library has a built-in function for converting MIDI objects into piano rolls. The code below uses librosa's specshow library to display a piano roll, which is nice because you can automatically have the y-axis display note names.\n",
        "\n",
        "In order to create a piano roll for a given MIDI, we need to choose a time subdivision to use for the x-axis. Since the MIDIs are standardized to 120bpm, we know that the length of 1 \"beat\" (typically a quarter note) is 1/2 a second. We will choose a time subdivision of 1/16th of a second, which will allow us to represent 32nd notes.\n",
        "\n",
        "1.5 Try using the function below to plot the first 15 seconds of the Mamma Mia MIDI object from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FTAR-_gnZei"
      },
      "outputs": [],
      "source": [
        "def plot_piano_roll(pm, length, fs=16):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # Use librosa's specshow function for displaying the piano roll\n",
        "    pianoroll = pm.get_piano_roll(fs) #=fs)\n",
        "    pianoroll[pianoroll > 0] = 1 # sets all the velocities to be the same\n",
        "    pianoroll = pianoroll[:, :fs*length]\n",
        "    nonzero_row_indices = np.nonzero(np.count_nonzero(pianoroll, axis=1))\n",
        "    start_pitch = max(np.min(nonzero_row_indices) - 3, 0)\n",
        "    end_pitch = min(np.max(nonzero_row_indices) + 3, 127)\n",
        "    librosa.display.specshow(pianoroll[start_pitch:end_pitch],\n",
        "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
        "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
        "\n",
        "\n",
        "### YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoEZA1iNtcNi"
      },
      "source": [
        "It would also be nice to be listen to our MIDIs! We can do that using pretty_midi's fluidsynth method.\n",
        "\n",
        "1.6 Use the code below to render 60 seconds of audio for our Mamma Mia MIDI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEmYVcilxe25"
      },
      "outputs": [],
      "source": [
        "def display_audio(pm, seconds=None, fs=16000):\n",
        "  waveform = pm.fluidsynth(fs=fs)\n",
        "  if seconds:\n",
        "    waveform = waveform[:seconds*fs]\n",
        "  return Audio(waveform, rate=fs)\n",
        "\n",
        "### YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ieUznzwtys"
      },
      "source": [
        "### Part 2: Creating our Representation. (15 points)\n",
        "An LSTM will not be able to understand a MIDI object or a piano roll object. LSTMs take as input sequences of integers, so we need to convert our MIDIs into this kind of representation. Since we are working strictly with monophonic MIDIs, there is a very natural way to represent our dataset. We will create a dictionary in which each integer key corresponds to a distinct pitch (0-127) and then add three extra tokens for rests, beginning-of-sequence, and end-of-sequence.\n",
        "\n",
        "2.1 Define a token dictionary with integer keys to represent the 128 MIDI pitches, a `<rest>` token, a `<bos>` token, and an `<eos>` token. Create an inverse token dictionary to be used for going from sequence representation back to MIDI. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5KfteGSwHuz"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "# Need a token dictionary and an inverse token dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwbYCfP03mAa"
      },
      "source": [
        "We will also chunk the MIDI data into shorter segments to make everything more manageable. Let's consider chunks that are 4 bars long. With a 32nd note subdivision, each chunk will be 128 tokens long, plus a `<bos>` and `<eos>` token, so 130 total. We will want to exclude chunks that are majority rests. Let's write some code to create our token sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Aq_xD4i37Bm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def create_token_sequences(midi, token_dict, fs=16, seq_len=128, add_bos_eos=True):\n",
        "  pianoroll = midi.get_piano_roll(fs)\n",
        "  pianoroll[pianoroll>0]=1\n",
        "  total_len = pianoroll.shape[1] // 128\n",
        "  seqs = []\n",
        "  for i in range(total_len):\n",
        "    vel_sum = np.sum(pianoroll[:,i*seq_len:(i+1)*seq_len])\n",
        "    if vel_sum > seq_len*(0.3):\n",
        "      new_seq = []\n",
        "      if add_bos_eos:\n",
        "        new_seq.append(token_dict['<bos>'])\n",
        "      for j in range(seq_len):\n",
        "        nonzero_indices = np.nonzero(pianoroll[:,i*seq_len + j])\n",
        "        if len(nonzero_indices[0]) != 0:\n",
        "          new_seq.append(nonzero_indices[0][0])\n",
        "        else:\n",
        "          new_seq.append(token_dict['<rest>'])\n",
        "      if add_bos_eos:\n",
        "        new_seq.append(token_dict['<eos>'])\n",
        "      seqs.append(new_seq)\n",
        "  return seqs\n",
        "\n",
        "def create_all_token_sequences(data_dir, token_dict, fs=16, seq_len=128, test_prop=0.10):\n",
        "  file_names = os.listdir(data_directory)\n",
        "  all_sequences = []\n",
        "  for f in tqdm(file_names):\n",
        "    pm = pretty_midi.PrettyMIDI(data_directory + f)\n",
        "    cur_seqs = create_token_sequences(pm, token_dict, fs=fs, seq_len=seq_len)\n",
        "    all_sequences += cur_seqs\n",
        "\n",
        "  # shuffle list\n",
        "  random.shuffle(all_sequences)\n",
        "  test_seqs = all_sequences[:int(len(all_sequences)*test_prop)]\n",
        "  train_seqs = all_sequences[int(len(all_sequences)*test_prop):]\n",
        "  return train_seqs, test_seqs\n",
        "\n",
        "### YOUR CODE HERE\n",
        "# Call create_all_token_sequences to create test and train sequences\n",
        "train_seqs, test_seqs = [],[]\n",
        "print(len(train_seqs), len(test_seqs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecgQiuomL1Hv"
      },
      "source": [
        "What if we want to go back the other way? We need an inverse function so we can convert any sequences that our model generates into MIDI objects! It's pretty straightforward to go from sequence to pianoroll. Going from pianoroll to MIDI is a little more complicated, so we provided code for that.\n",
        "\n",
        "2.2 Finish the function below by creating a pianoroll given a sequence of pitch tokens. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWXJl86TL0Tp"
      },
      "outputs": [],
      "source": [
        "def piano_roll_to_pretty_midi(piano_roll, fs=16, program=0):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    piano_roll : np.ndarray\n",
        "    fs : int\n",
        "        Sampling frequency of the columns\n",
        "    Returns\n",
        "    -------\n",
        "    midi_object : pretty_midi.PrettyMIDI\n",
        "    '''\n",
        "    notes, frames = piano_roll.shape\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(program=program)\n",
        "\n",
        "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
        "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
        "\n",
        "    # use changes in velocities to find note on / note off events\n",
        "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
        "\n",
        "    # keep track on velocities and note on times\n",
        "    prev_velocities = np.zeros(notes, dtype=int)\n",
        "    note_on_time = np.zeros(notes)\n",
        "\n",
        "    for time, note in zip(*velocity_changes):\n",
        "        # use time + 1 because of padding above\n",
        "        velocity = piano_roll[note, time + 1]\n",
        "        time = time / fs\n",
        "        if velocity > 0:\n",
        "            if prev_velocities[note] == 0:\n",
        "                note_on_time[note] = time\n",
        "                prev_velocities[note] = velocity\n",
        "        else:\n",
        "            pm_note = pretty_midi.Note(\n",
        "                velocity=prev_velocities[note],\n",
        "                pitch=note,\n",
        "                start=note_on_time[note],\n",
        "                end=time)\n",
        "            instrument.notes.append(pm_note)\n",
        "            prev_velocities[note] = 0\n",
        "    pm.instruments.append(instrument)\n",
        "    return pm\n",
        "\n",
        "def mono_seq_to_pretty_midi(seq, inv_token_dict):\n",
        "    ### YOUR CODE HERE! Take a sequence, create a pianoroll\n",
        "    #   to pass to piano_roll_to_pretty_midi\n",
        "    #   check for BOS, EOS!\n",
        "    pianoroll = None\n",
        "\n",
        "    midi = piano_roll_to_pretty_midi(pianoroll)\n",
        "\n",
        "    for note in midi.instruments[0].notes:\n",
        "        note.velocity = 100\n",
        "\n",
        "    return pianoroll,midi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsS6h1e5OwgY"
      },
      "outputs": [],
      "source": [
        "plot_piano_roll(pm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUFAyzKiONo6"
      },
      "outputs": [],
      "source": [
        "### Unit test: go from MIDI to pianoroll and plot. Then go to sequence, back to MIDI then plot\n",
        "import itertools\n",
        "\n",
        "tokens = create_token_sequences(pm, fs=16, seq_len=128, add_bos_eos=False)\n",
        "print(len(tokens))\n",
        "all_tokens = list(itertools.chain.from_iterable(tokens)) # concatenate all the token list chunks\n",
        "print(len(all_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAT7-m_VQEpx"
      },
      "outputs": [],
      "source": [
        "new_pr, new_pm = mono_seq_to_pretty_midi(all_tokens)\n",
        "plot_piano_roll(new_pm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrsVIlsOFL9d"
      },
      "source": [
        "### Part 3: LSTMs (30 Points)\n",
        "\n",
        "Now we have to define an autoregressive model. We have provided the init function and supporting weight initialization functions.\n",
        "\n",
        "3.1 Fill in the forward pass of the LSTM that should embed the source sequence, send it through the LSTM, and then generate a prediction with the fully connected layer. You can also add dropout layers to try to reduce overfitting/create a more robust model. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnI9-aa54LMO"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate,\n",
        "                tie_weights):\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
        "                    dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "        if tie_weights:\n",
        "            assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n",
        "            self.embedding.weight = self.fc.weight\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, src, hidden): # TO DO\n",
        "        prediction, hidden = None, None\n",
        "        return prediction, hidden\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_range_emb = 0.1\n",
        "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
        "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
        "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "        self.fc.bias.data.zero_()\n",
        "        for i in range(self.num_layers):\n",
        "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
        "                    self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
        "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim,\n",
        "                    self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        hidden = torch.zeros(self.num_layers, self.hidden_dim)\n",
        "        cell = torch.zeros(self.num_layers, self.hidden_dim)\n",
        "        return hidden, cell\n",
        "\n",
        "    def detach_hidden(self, hidden):\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach()\n",
        "        cell = cell.detach()\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liZHxesiN-18"
      },
      "source": [
        "**3.2** Define an LSTM and initialize the hidden states. Also define an Adam optimizer and use cross entropy loss as the criterion. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSiN7FayfcTD"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "model = None\n",
        "hidden = None\n",
        "optimizer = None\n",
        "criterion = None\n",
        "# Leave the rest of these alone\n",
        "batch_size = 1\n",
        "seq_len = 129\n",
        "best_test_loss = float('inf')\n",
        "best_model_params = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4GGnZU3OPZy"
      },
      "source": [
        "**3.3** Write a training loop for the model. Note that we are using a batch size of one. You can use `torch.nn.utils.clip_grad_norm_` with a clip value of 0.25 to try to improve performance. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfSSWdKE4LsP"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "for e in range(num_epochs): # TO DO\n",
        "  epoch_loss = 0\n",
        "  epoch_test_loss = 0\n",
        "  model.train()\n",
        "  for seq in tqdm(train_seqs):\n",
        "    # WRITE TRAINING LOOP\n",
        "    print(\"Training!\")\n",
        "\n",
        "  model.eval()\n",
        "  for seq in tqdm(test_seqs): # TO DO\n",
        "    # WRITE EVAL LOOP\n",
        "    print(\"Eval!\")\n",
        "\n",
        "  print(e, epoch_loss/len(train_seqs), epoch_test_loss/len(test_seqs))\n",
        "\n",
        "  if epoch_test_loss < best_test_loss:\n",
        "    print(\"MODEL UPDATED\")\n",
        "    best_test_loss = epoch_test_loss\n",
        "    best_model_params = {\n",
        "        'model_state_dict':model.state_dict(),\n",
        "        'optimizer_state_dict':optimizer.state_dict()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrNb0IT9Qv7S"
      },
      "source": [
        "**3.4** Finish this function that takes an LSTM model and the beginning of a sequence and generates a continuation of the sequence. You can divide predictions by the `temp` variable before using softmax and sampling to encourage more variation in the output. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqb9XfQw8kD_"
      },
      "outputs": [],
      "source": [
        "def lstm_generate(model, prompt, max_seq_len=130, temp=0.5, seed=None):\n",
        "    # BATCH SIZE MUST BE 1!!! #\n",
        "    model.eval()\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    indices = [t for t in prompt]\n",
        "    hidden = model.init_hidden(1) # batch_size = 1\n",
        "    with torch.no_grad():\n",
        "        while len(indices) < max_seq_len: # every time through this loop we predict the next token\n",
        "            # YOUR CODE HERE\n",
        "            prediction = None\n",
        "            if prediction == 130: #eos\n",
        "                break\n",
        "            indices.append(prediction)\n",
        "    return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KuUS_pXQvUS"
      },
      "outputs": [],
      "source": [
        "# TEST GENERATION!\n",
        "generation = lstm_generate(model, [129,80,80,80,80,60,60,60,60], max_seq_len=130, temp=0.5)\n",
        "print(generation)\n",
        "new_pr, new_pm = mono_seq_to_pretty_midi(generation[1:])\n",
        "plot_piano_roll(new_pm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcoOmmhYDA-M"
      },
      "outputs": [],
      "source": [
        "display_audio(new_pm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGG2eLQcx1KX"
      },
      "source": [
        "## Part 4 Transformers (30 points)\n",
        "\n",
        "4.1 Fill in the forward function of the transformer model below. You will need to embed the source sequence, add the positional encoding, send it through the transformer with a square causal mask (see `nn.Transformer.generate_square_subsequent_mask`), and then use a linear layer afterward to get predictions similar to the LSTM model. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2RL1Exax3En"
      },
      "outputs": [],
      "source": [
        "from torch import nn, Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "BOS_IDX, EOS_IDX = 129, 130 # 131\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.linear = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src): # TO DO\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            src: Tensor of shape ``[seq_len, batch_size]``\n",
        "        Returns:\n",
        "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
        "        \"\"\"\n",
        "        ### TO DO, don't forget src_mask!\n",
        "        output = None\n",
        "        return output\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ-3yTgVc-Pv"
      },
      "outputs": [],
      "source": [
        "### HELPER FUNCTION FOR BATCH STUFF ###\n",
        "def get_batch(train_seqs, i, bs=4):\n",
        "  seqs = []\n",
        "  for j in range(bs):\n",
        "    seqs.append(train_seqs[i*bs + j])\n",
        "  seqs = torch.LongTensor(seqs).T\n",
        "  source = seqs[:-1]\n",
        "  target = seqs[1:].reshape(-1)\n",
        "\n",
        "  return source, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwQwPUQdRk6z"
      },
      "source": [
        "**4.2** Define a transformer using the class constructor above. Also define an Adam optimizer and use cross entropy loss as the criterion. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L1vmtnORjcB"
      },
      "outputs": [],
      "source": [
        "ntokens = 131  # size of vocabulary\n",
        "emsize = 512  # embedding dimension\n",
        "d_hid = 512 # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
        "nlayers = 4  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
        "nhead = 8  # number of heads in ``nn.MultiheadAttention``\n",
        "dropout = 0.2  # dropout probability\n",
        "transformer = None\n",
        "criterion = None\n",
        "optimizer = None\n",
        "# Leave the rest of these alone\n",
        "bs = 1\n",
        "seq_len = 129\n",
        "num_epochs = 10\n",
        "best_test_loss = float('inf')\n",
        "best_model_params = None\n",
        "log_interval=200 # for progress updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brBDnCIGTWPY"
      },
      "source": [
        "**4.3** Write a training loop for the model. Use the `get_batch` function above to generate the data and targets for the predictions. You can use `torch.nn.utils.clip_grad_norm_` with a clip value of 0.25 to try to improve performance. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "66kQJDH84qH5"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "for e in range(num_epochs): # TO DO\n",
        "  epoch_loss = 0\n",
        "  epoch_test_loss = 0\n",
        "  transformer.train()\n",
        "  for i in range(len(train_seqs)):\n",
        "    ### TO DO: TRAINING LOOP ###\n",
        "\n",
        "    if i % log_interval == 0 and i > 0:\n",
        "      #lr = scheduler.get_last_lr()[0]\n",
        "      cur_loss = epoch_loss / i\n",
        "      print(f'| epoch {e:3d} | i {i:3d} |'\n",
        "            f'loss {cur_loss:5.2f}')\n",
        "\n",
        "  transformer.eval()\n",
        "  for i in tqdm(range(len(test_seqs))):\n",
        "    ### TO DO: EVAL LOOP ###\n",
        "    print(\"Eval!\")\n",
        "\n",
        "  print(e, epoch_loss/len(train_seqs), epoch_test_loss/len(test_seqs))\n",
        "\n",
        "  if epoch_test_loss < best_test_loss:\n",
        "    print(\"BEAT OLD ONES!\")\n",
        "    best_test_loss = epoch_test_loss\n",
        "    best_model_params = {\n",
        "        'model_state_dict':model.state_dict(),\n",
        "        'optimizer_state_dict':optimizer.state_dict()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpNmHEopT4d9"
      },
      "source": [
        "**4.4** Finish this function that takes a transformer model and the beginning of a sequence and generates a continuation of the sequence. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASPhwSLwgOHF"
      },
      "outputs": [],
      "source": [
        "def transformer_generate(model, prompt, max_seq_len, temp=1.0, seed=None):\n",
        "    # BATCH SIZE MUST BE 1 #\n",
        "    model.eval()\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    indices = [t for t in prompt]\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_seq_len-len(indices)):\n",
        "            ### YOUR CODE HERE\n",
        "            prediction=None\n",
        "            if prediction == 130: #eos\n",
        "                break\n",
        "            indices.append(prediction)\n",
        "    return indices\n",
        "\n",
        "generation = transformer_generate(transformer, [129,120,120,120], 65, temp=.8)\n",
        "print(generation)\n",
        "new_pr, new_pm = mono_seq_to_pretty_midi(generation[1:])\n",
        "plot_piano_roll(new_pm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1l19Kkintp_"
      },
      "outputs": [],
      "source": [
        "display_audio(new_pm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}